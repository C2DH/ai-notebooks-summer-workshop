{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Explain code and suggest code with llama7b\n",
        "\n",
        "This notebook uses a quantized version of LLama 7B."
      ],
      "metadata": {
        "id": "p-3AZ1cxQ84T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies and prepare helper code"
      ],
      "metadata": {
        "id": "n0Vy_PIpRMCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
        "\n",
        "!CT_CUBLAS=1 pip install ctransformers --no-binary ctransformers\n",
        "!pip install --quiet accelerate"
      ],
      "metadata": {
        "id": "HIlK03PPRLQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEZuiJQ_Q4xY"
      },
      "outputs": [],
      "source": [
        "# check GPU status\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def get_gpu_memory_usage():\n",
        "  \"\"\"\n",
        "  Returns the current free and used memory of the GPU in megabytes.\n",
        "  \"\"\"\n",
        "  device = torch.cuda.current_device()\n",
        "  properties = torch.cuda.get_device_properties(device)\n",
        "  free_memory, total_memory = torch.cuda.mem_get_info(device)\n",
        "  used_memory = total_memory - free_memory\n",
        "  return free_memory / 1024**2, used_memory / 1024**2\n",
        "\n",
        "free_memory, used_memory = get_gpu_memory_usage()\n",
        "print(f\"Free memory: {free_memory:.2f} MB\")\n",
        "print(f\"Used memory: {used_memory:.2f} MB\")\n",
        "print(torch.cuda.current_device())"
      ],
      "metadata": {
        "id": "TU4upjfzRj8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model."
      ],
      "metadata": {
        "id": "Z_u5u4CdRejB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ctransformers import AutoModelForCausalLM, AutoConfig\n",
        "\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GGUF\")\n",
        "config.max_seq_len = 2048 #4096\n",
        "config.max_answer_len= 1024\n",
        "\n",
        "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    \"TheBloke/Llama-2-7b-Chat-GGUF\",\n",
        "    model_file=\"llama-2-7b-chat.Q5_K_S.gguf\",\n",
        "    # model_file=\"llama-2-7b-chat.Q4_0.gguf\",\n",
        "    # model_file=\"llama-2-7b-chat.Q6_K.gguf\",\n",
        "    model_type=\"llama\",\n",
        "    gpu_layers=130,\n",
        "    context_length=4096,\n",
        "    max_new_tokens=2048,\n",
        "  )\n",
        "\n"
      ],
      "metadata": {
        "id": "U6m1mpjWRc-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check that the context length has been set\n",
        "llm.context_length"
      ],
      "metadata": {
        "id": "fvHDs440Rmut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Render helpers"
      ],
      "metadata": {
        "id": "yeeUWtvwRx2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def as_md(text):\n",
        "  return display(Markdown(text))"
      ],
      "metadata": {
        "id": "o4NZvWoPRyzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import markdown\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from typing import Callable\n",
        "\n",
        "AnswerSetter = Callable[[str], None]\n",
        "\n",
        "spinner = \"\"\"\n",
        "<div class=\"spinner\" style=\"display:inline-block;position:relative;width:80px;height:40px;\">\n",
        "  <div\n",
        "    style=\"display:inline-block;position:absolute;left:8px;width:16px;height:16px;border-radius:50%;background-color:#999;animation:bounce 2s infinite ease-in-out;\">\n",
        "  </div>\n",
        "  <div\n",
        "    style=\"display:inline-block;position:absolute;left:32px;width:16px;height:16px;border-radius:50%;background-color:#999;animation:bounce 2s infinite ease-in-out;animation-delay:-0.2s;\">\n",
        "  </div>\n",
        "  <div\n",
        "    style=\"display:inline-block;position:absolute;left:56px;width:16px;height:16px;border-radius:50%;background-color:#999;animation:bounce 2s infinite ease-in-out;animation-delay:-0.4s;\">\n",
        "  </div>\n",
        "</div>\n",
        "<style>\n",
        "  .spinner {\n",
        "    display: inline-block;\n",
        "    position: relative;\n",
        "    width: 80px;\n",
        "    height: 80px;\n",
        "  }\n",
        "\n",
        "  .spinner div {\n",
        "    display: inline-block;\n",
        "    position: absolute;\n",
        "    left: 8px;\n",
        "    width: 16px;\n",
        "    height: 16px;\n",
        "    border-radius: 50%;\n",
        "    background-color: #333;\n",
        "    animation: bounce 2s infinite ease-in-out;\n",
        "  }\n",
        "\n",
        "  .spinner div:nth-child(2) {\n",
        "    left: 32px;\n",
        "    animation-delay: -0.2s;\n",
        "  }\n",
        "\n",
        "  .spinner div:nth-child(3) {\n",
        "    left: 56px;\n",
        "    animation-delay: -0.4s;\n",
        "  }\n",
        "\n",
        "  @keyframes bounce {\n",
        "\n",
        "    0%,\n",
        "    80%,\n",
        "    100% {\n",
        "      transform: scale(0);\n",
        "    }\n",
        "\n",
        "    40% {\n",
        "      transform: scale(1);\n",
        "    }\n",
        "  }\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "def render_q_and_a(code: str) -> AnswerSetter:\n",
        "  w = widgets.HTML(value=\"\")\n",
        "\n",
        "  def _get_html(q: str, a: str):\n",
        "    return f\"\"\"\n",
        "    <table>\n",
        "      <tr>\n",
        "        <th style=\"border: 1px solid #999;\">Code</th>\n",
        "        <th style=\"border: 1px solid #999;\">Explanation</th>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td style=\"padding: 2rem;\">\n",
        "          <code style=\"display: block; white-space: pre-wrap; word-wrap: break-word; background: black; padding: 1rem; text-align: left; line-height: 1.3;\">{q}</code>\n",
        "        </td>\n",
        "        <td style=\"padding: 2rem; width: 60%;\">\n",
        "          {a}\n",
        "        </td>\n",
        "      </tr>\n",
        "    </table>\n",
        "    \"\"\"\n",
        "\n",
        "  def _answer_setter(answer: str):\n",
        "    explanation_html = markdown.markdown(answer)\n",
        "    w.value = _get_html(code, explanation_html)\n",
        "\n",
        "  w.value = _get_html(code, spinner)\n",
        "  display(w)\n",
        "\n",
        "  return _answer_setter\n",
        "\n",
        "\n",
        "\n",
        "code = \"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tabulate import tabulate\n",
        "\n",
        "# python\n",
        "def greet(name):\n",
        "    print(f\"Hello, {name}!\")\n",
        "\"\"\"\n",
        "\n",
        "answer = \"\"\"\n",
        "#python\n",
        "\n",
        "foo bar\n",
        "\"\"\"\n",
        "\n",
        "# set_answer = render_q_and_a(code)\n",
        "# sleep(2)\n",
        "# set_answer(answer)"
      ],
      "metadata": {
        "id": "GW-4SQ9FR18M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run code explainer"
      ],
      "metadata": {
        "id": "KOFIKr9uR5EL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def describe_code(code: str) -> str:\n",
        "  prompt = f\"\"\"[INST] <<SYS>>\n",
        "You can explain to a humanitarian any code provided with little use of tech and math term and also without getting too much into details.\n",
        "IMPORTANT: Your answers are short, precise, use markdown bullet points where appropriate and do not get too much into details. No lead-in, off topic and straight into explanation.\n",
        "<</SYS>>\n",
        "{code}[/INST]\"\"\"\n",
        "  return llm(prompt)"
      ],
      "metadata": {
        "id": "6zEi3OPOR74Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_1 = \"\"\"\n",
        "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(stop_words=stop_words)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([' '.join(tokenize(d)) for d in df['description']])\n",
        "svd = SVD(10)\n",
        "svd.fit(tfidf_matrix)\n",
        "\n",
        "def cosine(a, b):\n",
        "    eps = 1e-8\n",
        "    if type(a) is np.ndarray:\n",
        "        return a.dot(b) / ((np.linalg.norm(a) * np.linalg.norm(b)) + eps)\n",
        "    else:\n",
        "        return a.dot(b) / ((a.norm() * b.norm()) + eps)\n",
        "\n",
        "def tfidf_model(query, document):\n",
        "    query_vector = tfidf_vectorizer.transform([' '.join(tokenize(query, tfidf_vectorizer.get_feature_names()))]).todense()\n",
        "    doc_vector = tfidf_vectorizer.transform([' '.join(tokenize(document, tfidf_vectorizer.get_feature_names()))]).todense()\n",
        "    doc_vector = np.squeeze(np.asarray(doc_vector))\n",
        "    query_vector = np.squeeze(np.asarray(query_vector))\n",
        "    return cosine(query_vector, doc_vector)\n",
        "\"\"\"\n",
        "\n",
        "sample_2 = \"\"\"\n",
        "\n",
        "def lsa_model(query, document):\n",
        "    query = ' '.join(tokenize(query, tfidf_vectorizer.get_feature_names()))\n",
        "    document = ' '.join(tokenize(document, tfidf_vectorizer.get_feature_names()))\n",
        "    query_vector = tfidf_vectorizer.transform([query]).todense()\n",
        "    doc_vector = tfidf_vectorizer.transform([document]).todense()\n",
        "    query_vector = svd.transform(query_vector)\n",
        "    doc_vector = svd.transform(doc_vector)\n",
        "    return cosine(np.squeeze(np.asarray(doc_vector)), np.squeeze(np.asarray(query_vector)))\n",
        "\n",
        "def search(model, query, documents, names = None):\n",
        "    scores = [model(query, document) for document in documents]\n",
        "    ixs = list(reversed(np.argsort(scores)[-10:]))\n",
        "\n",
        "    data = [[scores[i] for i in ixs], [documents[i][:150] for i in ixs]]\n",
        "    if names is not None:\n",
        "        data.append([names[i] for i in ixs])\n",
        "    tabulate(data, header=False)\n",
        "\"\"\"\n",
        "as_md(describe_code(sample_1))"
      ],
      "metadata": {
        "id": "DIqLOKxwVIPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "raw_ipynb_url = 'https://raw.githubusercontent.com/C2DH/ai-notebooks-summer-workshop/master/examples/AI_Workshop_Semantic_Search.ipynb'\n",
        "ipynb_data = requests.get(raw_ipynb_url).json()\n",
        "\n",
        "code_cells_code = [''.join(c['source']) for c in ipynb_data['cells'] if c['cell_type'] == 'code']\n",
        "code_cells_code = [c for c in code_cells_code if c.replace('\\n', '').strip() != '']\n",
        "code_cells_code[1]"
      ],
      "metadata": {
        "id": "8y3-NwH4VOj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for code in code_cells_code:\n",
        "  set_answer = render_q_and_a(code)\n",
        "  description = describe_code(code)\n",
        "  set_answer(description)"
      ],
      "metadata": {
        "id": "-JfMvFQzVSe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code generator"
      ],
      "metadata": {
        "id": "xkgYwZ0ZWB_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_code(code: str) -> str:\n",
        "  prompt = f\"\"\"[INST] <<SYS>>\n",
        "You are a developer who can write Python code from an oral explanation. The code should use Impresso python library which exposes the following methods on the client instance `impresso`:\n",
        " * impresso.search.find(q=<query_term_in_any_language>, order_by=<'date', 'id', 'relevance'>)\n",
        " * impresso.articles.get(<article_id>)\n",
        "All methods return a Result object with the properties:\n",
        " * `data` - result as a dict\n",
        " * `pydantic` - as a pydantic model\n",
        " * `df` - as a pandas dataframe\n",
        "<</SYS>>\n",
        "{code}[/INST]\"\"\"\n",
        "  return llm(prompt)"
      ],
      "metadata": {
        "id": "6iQp_VVhWEsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = \"\"\"\n",
        "I need to get a list of the most recent articles that talk about nuclear power plants in German or French. I need a dataframe.\n",
        "\"\"\"\n",
        "code = generate_code(q)\n",
        "as_md(code)"
      ],
      "metadata": {
        "id": "DDVs23NUWLnO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}