{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af7fb10-dd81-4e43-b8fc-c1316e32dd49",
   "metadata": {},
   "source": [
    "# Generating Code with LLMs\n",
    "\n",
    "In this notebook, we will use the `transformers` library to generate text based on a given natural language prompt. We will utilize the `google/codegemma-7b` model for this purpose.\n",
    "\n",
    "## Step-by-Step Instructions\n",
    "\n",
    "### 1. Import the Required Libraries\n",
    "\n",
    "First, we need to import the necessary modules from the `transformers` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed75ed14-9a3b-4b77-b42f-cfd738f78148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d212ea-32bc-45fe-93d4-cd22138fe173",
   "metadata": {},
   "source": [
    "### 2. Load the Tokenizer and Model\n",
    "We will load the tokenizer and model using the specified model ID, `google/codegemma-7b` or `bigcode/starcoder`. Both LLMs are gated, this means that you need to have an HF account, go to the LLM page (i.e. [google/codegemma-7b](https://huggingface.co/google/codegemma-7b) and ask for access. This typically takes a couple of minutes (or even instantly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9d0fbf9-9d82-44f8-b788-60c5c9fbc0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [04:19<00:00, 37.06s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  6.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Replace model_id with the chosen LLM\n",
    "\n",
    "model_id = \"google/codegemma-7b\"\n",
    "# model_id = \"bigcode/starcoder\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093059d-e285-4300-bb15-bcab86486d4c",
   "metadata": {},
   "source": [
    "### 2. Define the Function to Generate Text\n",
    "Next, we define a function generate_text_from_prompt that takes a natural language prompt and generates text based on it. The function uses the loaded model to generate the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e751dc39-fd7b-46b4-8d60-ed1ffac56143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_prompt(prompt, max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Generates text from a given prompt using a specified model.\n",
    "\n",
    "    Args:\n",
    "    - prompt (str): The natural language prompt to generate text from.\n",
    "    - max_new_tokens (int): The maximum number of new tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated text based on the prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_len = inputs[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    # Generate text\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    generated_text = tokenizer.decode(outputs[0][prompt_len:])\n",
    "    \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96688e-9ab8-485e-97d8-f6a3d265dea7",
   "metadata": {},
   "source": [
    "### 4. Example Usage\n",
    "We can now use the function to generate text based on a user prompt. For example, let's generate a Python function that calculates the factorial of a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "377af64b-e70a-41c9-8d94-44ca4ee1d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# The factorial of a number is defined as the product of an integer and all the integers below it. For example, the factorial of five (5!) is equal to 120.\n",
      "\n",
      "# 5! = 5 x 4 x 3 x 2 x 1 = 120\n",
      "\n",
      "# The value of 0! is 1.\n",
      "\n",
      "# Example Input/Output 1:\n",
      "\n",
      "# Input: 5\n",
      "# Output: 120\n",
      "\n",
      "# Example Input/Output 2:\n",
      "\n",
      "# Input: 0\n",
      "# Output: 1\n",
      "\n",
      "# Example Input/Output 3:\n",
      "\n",
      "# Input: 1\n",
      "# Output: 1\n",
      "\n",
      "# Example Input/Output 4:\n",
      "\n",
      "# Input: 2\n",
      "# Output: 2\n",
      "\n",
      "# Example Input/Output 5:\n",
      "\n",
      "# Input: 3\n",
      "# Output: 6\n",
      "\n",
      "# Example Input/Output 6:\n",
      "\n",
      "# Input: 4\n",
      "# Output: 24\n",
      "\n",
      "# Example Input/Output 7:\n",
      "\n",
      "# Input: 5\n",
      "# Output: 120\n",
      "\n",
      "# Example Input/Output \n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "user_prompt = \"Write a function in Python that calculates the factorial of a number.\"\n",
    "generated_text = generate_text_from_prompt(user_prompt, max_new_tokens=256)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e375289-0312-4140-ace4-e1dcab3d655f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "user_prompt = \"Write a function in Python that analyzes a text corpus to find the most frequent words, excluding common stop words, and visualizes the results with a bar chart.\"\n",
    "generated_text = generate_text_from_prompt(user_prompt, max_new_tokens=256)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5aadf8-5efa-4928-b692-c5699055fe7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc3ec61-494e-455b-8c9d-10de75f512d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b63ee-4941-40d0-8962-1fa6429654fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a579b5-0fbf-4e92-a3ea-56a6e0fd964e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438748b0-d8a1-4b7f-bbb2-bc5ef348341b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d725f68-b417-44b5-b32b-eb7313a8bc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6614630-23ec-4030-9737-e57fdf949a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
